# D-PC Client Configuration - Default Settings (v0.10.0)
# This file shows the default configuration generated on first install in ~/.dpc/config.ini
#
# You can override these settings with environment variables using the format:
# DPC_SECTION_KEY (e.g., DPC_HUB_URL, DPC_OAUTH_CALLBACK_PORT, DPC_TURN_USERNAME, etc.)
#
# For complete documentation, see: docs/CONFIGURATION.md

# =============================================================================
# FEDERATION HUB SETTINGS
# =============================================================================
[hub]
# Hub URL for OAuth authentication and WebRTC signaling (optional in v0.10.0+)
# System works without Hub using direct connections, DHT, relays, and gossip
url = http://localhost:8000

# Auto-connect to Hub on startup (true) or start offline (false)
auto_connect = true

# =============================================================================
# OAUTH AUTHENTICATION
# =============================================================================
[oauth]
# OAuth callback server port for receiving authorization codes
callback_port = 8080

# OAuth callback server host (127.0.0.1 for localhost)
callback_host = 127.0.0.1

# Default OAuth provider: 'google' or 'github'
# Used for auto-connect when auto_connect = true
default_provider = google

# =============================================================================
# P2P CONNECTION SETTINGS
# =============================================================================
[p2p]
# TLS server listen port (8888 default)
listen_port = 8888

# Listen host: 'dual' (IPv4+IPv6), '0.0.0.0' (IPv4 only), '::' (IPv6 only)
listen_host = dual

# Connection establishment timeout in seconds
connection_timeout = 60

# =============================================================================
# LOCAL API SERVER (WebSocket for UI)
# =============================================================================
[api]
# WebSocket API server port for frontend communication
port = 9999

# API server host (127.0.0.1 for localhost only)
host = 127.0.0.1

# =============================================================================
# TURN SERVER (for WebRTC fallback)
# =============================================================================
[turn]
# TURN server username (optional, leave empty to use Google STUN only)
username =

# TURN server credential (optional)
credential =

# =============================================================================
# DEVICE CONTEXT COLLECTION
# =============================================================================
[system]
# Auto-collect device/system info for AI context (true recommended)
auto_collect_device_info = true

# Collect hardware tiers (RAM, CPU, disk, GPU) - privacy-rounded values
collect_hardware_specs = true

# Collect installed dev tools and versions (git, docker, node, etc.)
collect_dev_tools = true

# Collect locally available AI models (opt-in for compute-sharing)
# Set to true only if you plan to share compute resources with peers
collect_ai_models = false

# =============================================================================
# DHT PEER DISCOVERY (v0.10.0+)
# =============================================================================
[dht]
# Enable DHT for peer discovery (true recommended)
enabled = true

# UDP port for DHT RPCs (typically TLS port + 1)
port = 8889

# Kademlia k parameter: nodes per bucket (20 standard)
k = 20

# Parallelism factor for iterative lookups (3 standard)
alpha = 3

# Bootstrap timeout in seconds (30s recommended)
bootstrap_timeout = 30

# Lookup timeout in seconds (10s recommended)
lookup_timeout = 10

# Bucket refresh interval in seconds (3600 = 1 hour)
bucket_refresh_interval = 3600

# Re-announce interval in seconds (3600 = 1 hour)
announce_interval = 3600

# Comma-separated list of seed nodes for bootstrapping (ip:port format)
# Leave empty to use local network discovery
seed_nodes =

# =============================================================================
# CONNECTION STRATEGY (6-Tier Fallback Hierarchy) (v0.10.0+)
# =============================================================================
[connection]
# Priority 1: IPv6 Direct (40%+ networks, no NAT, 10s timeout)
enable_ipv6 = true

# Priority 2: IPv4 Direct (local network/port forward, 10s timeout)
enable_ipv4 = true

# Priority 3: Hub WebRTC with STUN/TURN (requires Hub, 30s timeout)
enable_hub_webrtc = true

# Priority 4: DHT-coordinated UDP hole punching (60-70% NAT, 15s timeout)
# ENABLED with DTLS encryption (v0.10.1+)
# Now safe to use - all hole-punched connections are encrypted with DTLS
enable_hole_punching = true

# Priority 5: Volunteer relay nodes (100% NAT coverage, 20s timeout)
enable_relays = true

# Priority 6: Gossip store-and-forward (disaster fallback, eventual delivery)
enable_gossip = true

# Per-strategy timeouts in seconds
ipv6_timeout = 10
ipv4_timeout = 10
webrtc_timeout = 30
hole_punch_timeout = 15
relay_timeout = 20
gossip_timeout = 5

# =============================================================================
# UDP HOLE PUNCHING (Priority 4) (v0.10.0+)
# =============================================================================
[hole_punch]
# UDP port for hole punching
udp_punch_port = 8890

# Enable NAT type detection (cone vs symmetric)
nat_detection_enabled = true

# Endpoint discovery timeout in seconds
stun_timeout = 5

# Number of punch attempts before giving up
punch_attempts = 3

# DTLS ENCRYPTION SETTINGS (v0.10.1+)
# All hole-punched connections are now encrypted with DTLS (Datagram TLS)
enable_dtls = true
dtls_handshake_timeout = 3  # Seconds for DTLS handshake
dtls_version = 1.2  # DTLS protocol version (1.2 or 1.3)

# =============================================================================
# VOLUNTEER RELAY NODES (Priority 5) (v0.10.0+)
# =============================================================================
[relay]
# CLIENT MODE: Use relays for outbound connections
enabled = true

# Preferred geographic region: 'us-west', 'eu-central', 'ap-southeast', 'global'
prefer_region = global

# Relay discovery cache timeout in seconds (300 = 5 minutes)
cache_timeout = 300

# SERVER MODE: Volunteer as relay for others (opt-in)
# Set to true only if you have stable internet and want to help the community
volunteer = false

# Maximum concurrent relay sessions (server mode)
max_peers = 10

# Bandwidth limit for relaying in Mbps (server mode)
bandwidth_limit_mbps = 10.0

# Geographic region for relay announcements (server mode)
region = global

# PRIVACY NOTE: Relays forward encrypted payloads only. They can see:
# - Peer node IDs (who is talking to whom)
# - Message sizes (payload size in bytes)
# - Message timing (when messages are sent)
# Relays CANNOT see: message content, conversation context, personal data

# =============================================================================
# GOSSIP STORE-AND-FORWARD (Priority 6) (v0.10.0+)
# =============================================================================
[gossip]
# Enable gossip protocol for disaster-resilient messaging
enabled = true

# Maximum hops for message forwarding (5 = reasonable reach)
max_hops = 5

# Number of random peers to forward to (3 = epidemic spreading)
fanout = 3

# Message TTL in seconds (86400 = 24 hours)
ttl_seconds = 86400

# Anti-entropy sync interval in seconds (300 = 5 minutes)
# Periodic reconciliation using vector clocks
sync_interval = 300

# Expired message cleanup interval in seconds (600 = 10 minutes)
cleanup_interval = 600

# Default message priority: 'low', 'normal', 'high'
priority = normal

# =============================================================================
# KNOWLEDGE COMMIT SYSTEM (v0.9.0+)
# =============================================================================
[knowledge]
# Warn when context window reaches this threshold (0.8 = 80%)
token_warning_threshold = 0.8

# Automatically suggest knowledge extraction when threshold reached
auto_extraction_enabled = true

# Include cultural perspective analysis in knowledge extraction
# (increases extraction time but improves global context awareness)
cultural_perspectives_enabled = false

# =============================================================================
# FILE TRANSFER SETTINGS (v0.11.0+)
# =============================================================================
[file_transfer]
# Chunk size for file transfers in bytes (65536 = 64KB)
# Smaller chunks = more overhead, larger chunks = less responsive progress
chunk_size = 65536

# Delay between sending chunks in seconds (0.001 = 1ms)
# Prevents flooding receiver, allows rate limiting for bandwidth control
# Set to 0 for maximum speed, increase for slower connections
chunk_delay = 0.001

# Threshold for background transfer in MB (files larger than this transfer in background)
# Background transfers don't block the UI during send/receive
background_threshold_mb = 50

# Threshold for direct TLS preference in MB (files larger than this prefer direct connections)
# Direct TLS is more efficient for large files than WebRTC/relay
direct_tls_only_threshold_mb = 100

# Maximum concurrent file transfers (sending + receiving combined)
max_concurrent_transfers = 3

# Verify file hash after transfer (SHA256)
# Recommended: true (ensures file integrity, minimal performance impact)
verify_hash = true

# =============================================================================
# VISION API SETTINGS (Phase 2: Screenshot + Vision Integration)
# =============================================================================
[vision]
# Enable vision API features (screenshot paste, image analysis)
enabled = true

# Default AI provider for vision analysis: 'openai' or 'anthropic'
# Used for AI chat auto-vision when pasting images
default_provider = openai

# Maximum image size in MB for clipboard paste and file uploads
# Images larger than this will be rejected
max_image_size_mb = 5

# Thumbnail JPEG quality (0-100)
# Higher quality = larger thumbnail file size
# 85 is recommended (good quality, reasonable size)
thumbnail_quality = 85

# =============================================================================
# VOICE MESSAGES (v0.13.0+)
# =============================================================================
[voice_messages]
# Enable voice message recording and playback
enabled = true

# Maximum recording duration in seconds (300 = 5 minutes)
max_duration_seconds = 300

# Maximum voice message file size in MB
max_size_mb = 10

# Supported audio formats (comma-separated)
mime_types = audio/webm,audio/opus,audio/ogg,audio/mp4,audio/mpeg,audio/wav

# Default sample rate in Hz (48000 = 48kHz for quality)
default_sample_rate = 48000

# Default audio channels (1 = mono, 2 = stereo)
default_channels = 1

# Default audio codec (opus for web compatibility)
default_codec = opus

# =============================================================================
# VOICE TRANSCRIPTION (v0.13.2+)
# =============================================================================
[voice_transcription]
# Enable auto-transcription of received voice messages
enabled = true

# Should sender transcribe their own voice messages before sending
# When true: sender transcribes, results sent to recipients (saves recipient compute)
# When false: recipients transcribe locally (coordination via recipient_delay_seconds)
sender_transcribes = false

# Wait N seconds before recipients attempt transcription (coordination)
# Prevents all recipients from transcribing simultaneously (wasteful)
# First recipient transcribes, result shared via broadcast message
recipient_delay_seconds = 3

# Max wait time for peer's transcription before trying locally (in seconds)
# Increased to 240s for cold model loads that take 180+s
timeout_seconds = 240

# Provider priority (comma-separated, overrides voice_provider from providers.json)
# Provider aliases match HuggingFace model names for clarity:
# - whisper-large-v3: openai/whisper-large-v3 (full model, ~3GB, best accuracy)
# - whisper-large-v3-turbo: openai/whisper-large-v3-turbo (faster, ~1.6GB)
# - whisper-medium: openai/whisper-medium (balance, ~1.5GB)
# - whisper-small: openai/whisper-small (fastest, ~500MB)
# - openai: OpenAI Whisper API (requires API key, most accurate)
# System tries each in order until one succeeds
provider_priority = whisper-large-v3,whisper-large-v3-turbo,whisper-medium,whisper-small,openai

# Show who transcribed the message in UI (e.g., "Transcribed by Alice")
show_transcriber_name = false

# Cache transcriptions in memory (avoid re-transcribing same message)
cache_transcriptions = true

# Fallback to OpenAI API if local Whisper unavailable
# Requires OpenAI API key in providers.json
fallback_to_openai = true

# =============================================================================
# LOCAL TRANSCRIPTION (v0.13.1+)
# =============================================================================
[local_transcription]
# Enable local Whisper transcription (PyTorch/MLX backend)
enabled = true

# Model name (HuggingFace)
# Recommended: openai/whisper-large-v3 for accuracy
# Alternatives: openai/whisper-medium, openai/whisper-small, openai/whisper-base
model = openai/whisper-large-v3

# Device: 'cuda', 'cpu', or 'auto' (auto-detects CUDA)
device = auto

# Use torch.compile for 4.5x speedup (PyTorch 2.4+, ignored on older versions)
compile_model = true

# Use Flash Attention 2 (requires flash-attn package, faster on NVIDIA GPUs)
use_flash_attention = false

# Chunk length for long-form transcription in seconds (speed vs accuracy)
# Higher = faster but less accurate for very long audio
chunk_length_s = 30

# Batch size for chunked transcription (higher = faster, more VRAM)
# Reduce if you get OOM errors: 16 (12GB VRAM), 8 (6GB VRAM), 1 (CPU)
batch_size = 16

# Language: 'auto' (detect) or ISO 639-1 code (e.g., 'en', 'es', 'zh')
language = auto

# Task: 'transcribe' (original language) or 'translate' (to English)
task = transcribe

# Fallback to OpenAI API if local transcription fails
fallback_to_openai = true

# Max audio file size for local transcription in MB (VRAM limit)
max_file_size_mb = 25

# Load model on first use (faster startup, lazy loading)
lazy_loading = true

# =============================================================================
# TELEGRAM INTEGRATION (v0.15.0+)
# =============================================================================
[telegram]
# Enable Telegram bot integration
enabled = false

# Telegram bot token (get from @BotFather on Telegram)
# Leave empty to disable Telegram integration
bot_token =

# Bot owner contact information (shown to unauthorized users)
# This helps users know how to request access to your private bot
# Examples:
# owner_contact = John Doe - Telegram: @johndoe, Email: john@example.com
# owner_contact = Contact me on Telegram @myusername
# owner_contact = Email me at admin@example.com for access
owner_contact =

# Custom access denied message (optional, overrides default)
# Leave empty to use default message with user instructions
# Available placeholders: {user_name}, {user_id}, {username}, {owner_contact}
# access_denied_message = ⚠️ Access Denied\n\nHello {user_name}!\n\nYour User ID: {user_id}\n\nContact {owner_contact} to request access.

# Enable voice message transcription (uses default voice provider from providers.json)
transcription_enabled = true

# =============================================================================
# HISTORICAL MESSAGE FETCHING (v0.15.0+)
# =============================================================================

# Fetch historical messages on bot startup
# When true: retrieves messages sent while DPC was offline (within 24 hours)
# When false: only processes new messages sent after bot starts
fetch_history_on_startup = true

# Maximum number of historical messages to fetch per chat (1-100)
# Telegram API limit: 100 messages per request
# Set lower to reduce startup time, higher to retrieve more history
history_fetch_limit = 100

# Maximum age of messages to fetch in hours (1-24)
# Telegram API hard limit: only messages from last 24 hours are available
# Messages older than 24 hours are permanently deleted from Telegram servers
history_max_age_hours = 24

# Message types to fetch during history sync (comma-separated)
# Available types: text, voice, photo, document, video
# Set to "text" only to skip media downloads (faster startup)
history_message_types = text,voice,photo,document,video

# Drop pending updates on startup
# When true: discards any pending updates (may lose messages sent while offline)
# When false: fetches pending updates (recommended when history fetching is enabled)
# Note: This setting is ignored if fetch_history_on_startup = true
drop_pending_updates = false

# Track last processed update_id per chat (JSON object, internal use)
# Prevents duplicate message processing across bot restarts
# Format: {"123456789": 12345, "987654321": 67890}
last_update_id = {}

# =============================================================================
# TELEGRAM BRIDGE MODE (v0.15.0+)
# =============================================================================

# Forward Telegram messages to P2P peers (bridge mode)
#
# CURRENT BEHAVIOR (v0.15.0):
# - Forwards to ALL connected P2P peers as individual 1:1 messages
# - Creates separate conversations with each peer (N threads for N peers)
# - No cross-peer visibility (Peer A can't see Peer B's messages)
# - Responses go to bridge owner, not the group
#
# LIMITATIONS:
# - Not suitable for team collaboration (no shared thread)
# - Inefficient: O(n) network cost instead of O(1)
# - No proper attribution (shows as your message, not "From Telegram")
#
# FUTURE ENHANCEMENT (Group Chat Phase):
# When DPC implements multiparty conversations, this will support:
# - bridge_target_type = "all_peers" | "group"
# - bridge_target_group_id = "my-team-group"
# - Single message to DPC group chat (O(1) network cost)
# - Proper Telegram sender attribution
# - Shared thread with cross-peer visibility
#
# Use case examples:
# - Bridge Telegram group chat → DPC P2P team
# - Include mobile-only team members in DPC conversations
# - Archive Telegram conversations in DPC (E2E encrypted storage)
bridge_to_p2p = false

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
[logging]
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
level = INFO

# Enable console output
console = true

# Console log level (can differ from file log level)
console_level = INFO

# Log file path (~ expands to user home directory)
file = ~/.dpc/logs/dpc-client.log

# Maximum bytes per log file before rotation (10485760 = 10MB)
max_bytes = 10485760

# Number of backup log files to keep
backup_count = 5
