{
  "_comment": "D-PC AI Providers Configuration - Manage your local and cloud AI providers. Copy this to ~/.dpc/providers.json and customize.",

  "default_provider": "ollama_text",
  "vision_provider": "ollama_vision",
  "voice_provider": "local_whisper",

  "providers": [
    {
      "_comment": "Local Ollama text provider - Fast model for regular chat queries",
      "alias": "ollama_text",
      "type": "ollama",
      "model": "llama3.1:8b",
      "host": "http://127.0.0.1:11434",
      "context_window": 131072
    },
    {
      "_comment": "Local Ollama vision provider - For image analysis queries",
      "alias": "ollama_vision",
      "type": "ollama",
      "model": "llama3.2-vision:11b",
      "host": "http://127.0.0.1:11434",
      "context_window": 131072
    },
    {
      "_comment": "Local Whisper transcription provider - Privacy-first voice messages (v0.13.1+)",
      "alias": "local_whisper",
      "type": "local_whisper",
      "model": "openai/whisper-large-v3",
      "device": "auto",
      "compile_model": true,
      "use_flash_attention": false,
      "chunk_length_s": 30,
      "batch_size": 16,
      "language": "auto",
      "task": "transcribe",
      "lazy_loading": true,
      "_note": "Privacy-first local transcription with OpenAI API fallback",
      "_performance": "RTX 3060 12GB: ~6s for 1-minute audio (10x real-time)",
      "_requirements": "CUDA GPU recommended (CPU fallback available, slower)"
    },
    {
      "_comment": "Local LM Studio - OpenAI-compatible API",
      "alias": "lm_studio",
      "type": "openai_compatible",
      "model": "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF",
      "base_url": "http://127.0.0.1:1234/v1",
      "api_key": "lm-studio"
    }
  ],

  "_examples": {
    "_comment": "Example configurations for cloud providers - Add these to the providers array above",

    "openai": {
      "alias": "gpt4_omni",
      "type": "openai_compatible",
      "model": "gpt-4o",
      "base_url": "https://api.openai.com/v1",
      "api_key_env": "OPENAI_API_KEY",
      "context_window": 128000,
      "_setup": "Set environment variable: export OPENAI_API_KEY='sk-...'"
    },

    "anthropic": {
      "alias": "claude_sonnet",
      "type": "anthropic",
      "model": "claude-3-5-sonnet-20240620",
      "api_key_env": "ANTHROPIC_API_KEY",
      "context_window": 200000,
      "_setup": "Set environment variable: export ANTHROPIC_API_KEY='sk-ant-...'"
    },

    "zai": {
      "alias": "zai_glm47",
      "type": "zai",
      "model": "glm-4.7",
      "api_key_env": "ZAI_API_KEY",
      "context_window": 128000,
      "_setup": "Set environment variable: export ZAI_API_KEY='your_key_here'",
      "_available_models": {
        "text": ["glm-4.7", "glm-4.6", "glm-4.5", "glm-4.5-air", "glm-4.5-airx", "glm-4.5-flash", "glm-4-plus", "glm-4-128-0414-128k"],
        "vision": ["glm-4.6v-flash", "glm-4.5v", "glm-4.0v"]
      },
      "_rate_limits": "Concurrency-based (2-20 concurrent requests depending on model)"
    },

    "openrouter": {
      "alias": "openrouter",
      "type": "openai_compatible",
      "model": "anthropic/claude-3.5-sonnet",
      "base_url": "https://openrouter.ai/api/v1",
      "api_key_env": "OPENROUTER_API_KEY",
      "_setup": "Set environment variable: export OPENROUTER_API_KEY='sk-or-...'"
    },

    "local_whisper_large": {
      "alias": "local_whisper_large",
      "type": "local_whisper",
      "model": "openai/whisper-large-v3",
      "device": "cuda",
      "compile_model": true,
      "use_flash_attention": false,
      "chunk_length_s": 30,
      "batch_size": 16,
      "language": "auto",
      "task": "transcribe",
      "lazy_loading": true,
      "_note": "NVIDIA GPU transcription (1.55B params, 99 languages, ~10-13x real-time)"
    },

    "apple_whisper_mlx": {
      "alias": "apple_whisper_mlx",
      "type": "local_whisper",
      "model": "openai/whisper-large-v3",
      "device": "mlx",
      "compile_model": false,
      "use_flash_attention": false,
      "chunk_length_s": 30,
      "batch_size": 16,
      "language": "auto",
      "task": "transcribe",
      "lazy_loading": true,
      "_note": "Apple Silicon GPU transcription (M1/M2/M3/M4, ~10-15x real-time)",
      "_setup": "Install MLX dependencies: poetry install -E mlx (macOS only)"
    }
  },

  "_field_reference": {
    "_comment": "Reference guide for provider configuration fields",
    "common_fields": {
      "alias": "User-friendly name for the provider (must be unique)",
      "type": "Provider type: 'ollama', 'openai_compatible', 'anthropic', 'zai', or 'local_whisper'",
      "model": "Model identifier (e.g., 'llama3.1:8b', 'gpt-4o', 'claude-3-5-sonnet-20240620', 'glm-4.7', 'openai/whisper-large-v3')",
      "context_window": "Optional: Override default context window size in tokens"
    },
    "ollama_fields": {
      "host": "Ollama API endpoint (default: http://127.0.0.1:11434)"
    },
    "openai_compatible_fields": {
      "base_url": "API endpoint URL",
      "api_key": "API key (plaintext - only use for local providers)",
      "api_key_env": "Environment variable containing API key (recommended for cloud)"
    },
    "anthropic_fields": {
      "api_key_env": "Environment variable containing Anthropic API key (required)"
    },
    "zai_fields": {
      "api_key": "API key (plaintext - alternative to api_key_env)",
      "api_key_env": "Environment variable containing Z.AI API key (recommended)"
    },
    "local_whisper_fields": {
      "device": "Device to use: 'cuda', 'cpu', or 'auto' (auto-detects CUDA)",
      "compile_model": "Enable torch.compile for 4.5x speedup (true/false)",
      "use_flash_attention": "Enable Flash Attention 2 for 20% additional speedup (requires flash-attn package)",
      "chunk_length_s": "Chunk length for long-form transcription in seconds (default: 30)",
      "batch_size": "Batch size for chunked transcription (higher = faster, more VRAM)",
      "language": "Language code or 'auto' for auto-detection",
      "task": "Task: 'transcribe' or 'translate' (to English)",
      "lazy_loading": "Load model on first use for faster startup (true/false)"
    }
  },

  "_security_notes": {
    "_comment": "Best practices for API key management",
    "recommendations": [
      "Use 'api_key_env' for cloud providers (stores keys in environment variables)",
      "Use 'api_key' only for local providers where security is not a concern",
      "Never commit API keys to version control",
      "Set environment variables in your shell profile (~/.bashrc, ~/.zshrc)",
      "On Windows, use System Environment Variables or PowerShell profile"
    ],
    "example_setup": {
      "linux_mac": "Add to ~/.bashrc or ~/.zshrc: export OPENAI_API_KEY='sk-...'",
      "windows_cmd": "setx OPENAI_API_KEY \"sk-...\"",
      "windows_powershell": "$env:OPENAI_API_KEY='sk-...'; [Environment]::SetEnvironmentVariable('OPENAI_API_KEY', 'sk-...', 'User')"
    }
  }
}
