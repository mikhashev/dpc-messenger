# ~/.dpc/providers.toml
#
# This is an example configuration for your AI providers.
# Copy this file to ~/.dpc/providers.toml and edit it with your details.

# The alias of the provider to use by default for local queries.
default_provider = "ollama_local"

# --- Provider Definitions ---

[[providers]]
  # A user-defined alias for this provider.
  alias = "ollama_local"
  # The type of provider determines which driver to use.
  type = "ollama"
  # Provider-specific settings.
  model = "llama3.1:8b"
  host = "http://127.0.0.1:11434"

[[providers]]
  alias = "lm_studio"
  type = "openai_compatible"
  model = "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF" # Example model name in LM Studio
  base_url = "http://127.0.0.1:1234/v1"
  # An API key is not needed for local LM Studio, but the library might require a placeholder.
  api_key = "lm-studio"

# --- Examples for Cloud Providers (Disabled by default) ---
# To use these, you must set the corresponding environment variables with your API keys.

# [[providers]]
#   alias = "gpt4_omni"
#   type = "openai_compatible"
#   model = "gpt-4o"
#   base_url = "https://api.openai.com/v1"
#   # The API key will be securely retrieved from this environment variable.
#   api_key_env = "OPENAI_API_KEY"

# [[providers]]
#   alias = "claude_sonnet"
#   type = "anthropic"
#   model = "claude-3-5-sonnet-20240620"
#   api_key_env = "ANTHROPIC_API_KEY"